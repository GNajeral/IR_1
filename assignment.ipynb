{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import string as st\n",
    "import re\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Input data files are available in the read-only \"./input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data. Here it is already in .csv format.\n",
    "train_data = pd.read_csv('dataset/BBC News Train.csv')\n",
    "test_data = pd.read_csv('dataset/BBC News Test.csv')\n",
    "data = pd.concat([train_data,test_data])\n",
    "data.to_csv('dataset/data.csv', index=False)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning and processing steps\n",
    "* Remove punctuations\n",
    "* Convert text to tokens\n",
    "* Remove tokens of length less than or equal to 3\n",
    "* Remove stopwords using NLTK corpus stopwords list to match\n",
    "* Apply lemmatization\n",
    "* Convert words to feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuations from the text\n",
    "\n",
    "def remove_punct(text):\n",
    "    return (\"\".join([ch for ch in text if ch not in st.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['removed_punc'] = data['Text'].apply(lambda x: remove_punct(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Convert text to lower case tokens. Here, split() is applied on white-spaces. But, it could be applied\n",
    "    on special characters, tabs or any other string based on which text is to be seperated into tokens.\n",
    "'''\n",
    "def tokenize(text):\n",
    "    text = re.split('\\s+' ,text)\n",
    "    return [x.lower() for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['removed_punc'].apply(lambda msg : tokenize(msg))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tokens of length less than 3\n",
    "\n",
    "def remove_small_words(text):\n",
    "    return [x for x in text if len(x) > 3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['larger_tokens'] = data['tokens'].apply(lambda x : remove_small_words(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Remove stopwords. Here, NLTK corpus list is used for a match. However, a customized user-defined \n",
    "    list could be created and used to limit the matches in input text. \n",
    "'''\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data['clean_tokens'] = data['larger_tokens'].apply(lambda x : remove_stopwords(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Apply lemmatization on tokens\n",
    "def lemmatize(text):\n",
    "    word_net = WordNetLemmatizer()\n",
    "    return [word_net.lemmatize(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data['lemma_words'] = data['clean_tokens'].apply(lambda x : lemmatize(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create sentences to get clean text as input for vectors\n",
    "\n",
    "def return_sentences(tokens):\n",
    "    return \" \".join([word for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data['clean_text'] = data['lemma_words'].apply(lambda x : return_sentences(x))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model and Evaluation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def balance_data(data, category_col):\n",
    "    categories = data[category_col].unique()\n",
    "    min_category_count = data[category_col].value_counts().min()\n",
    "\n",
    "    balanced_data = []\n",
    "\n",
    "    for category in categories:\n",
    "        category_data = data[data[category_col] == category]\n",
    "        category_data_balanced = resample(category_data, replace=False, n_samples=min_category_count, random_state=42)\n",
    "        balanced_data.append(category_data_balanced)\n",
    "\n",
    "    return pd.concat(balanced_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "balanced_data = balance_data(data, 'Category')\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_data['clean_text'], balanced_data['Category'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_counts = y_train.value_counts()\n",
    "print(\"Category counts in the training set:\")\n",
    "print(y_train_counts)\n",
    "\n",
    "y_test_counts = y_test.value_counts()\n",
    "print(\"\\nCategory counts in the testing set:\")\n",
    "print(y_test_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer()\n",
    "#X_train_vec = vectorizer.fit_transform(X_train)\n",
    "#X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# OR\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.9, min_df=5)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = y_train.unique()\n",
    "category_avg_vecs = {}\n",
    "\n",
    "for category in categories:\n",
    "    category_indices = y_train[y_train == category].index\n",
    "    category_vectors = X_train_vec[category_indices, :]\n",
    "    category_avg_vec = np.mean(category_vectors, axis=0)\n",
    "    category_avg_vecs[category] = category_avg_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def predict_category(text, vectorizer, category_avg_vecs):\n",
    "    text_vec = vectorizer.transform([text])\n",
    "    max_similarity = -1\n",
    "    predicted_category = None\n",
    "\n",
    "    for category, avg_vec in category_avg_vecs.items():\n",
    "        similarity = cosine_similarity(text_vec, np.asarray(avg_vec)) # Convert avg_vec to a numpy array\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            predicted_category = category\n",
    "\n",
    "    return predicted_category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [predict_category(text, vectorizer, category_avg_vecs) for text in X_test]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(classifier, X_train_vec, y_train, X_test_vec, y_test):\n",
    "    classifier.fit(X_train_vec, y_train)\n",
    "    y_pred = classifier.predict(X_test_vec)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    print(classifier.__class__.__name__)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    performance = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": classification_report_dict[\"macro avg\"][\"precision\"],\n",
    "        \"recall\": classification_report_dict[\"macro avg\"][\"recall\"],\n",
    "        \"f1_score\": classification_report_dict[\"macro avg\"][\"f1-score\"],\n",
    "    }\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics = {}\n",
    "\n",
    "classifiers = [\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    RandomForestClassifier(),\n",
    "    KNeighborsClassifier()\n",
    "]\n",
    "\n",
    "\n",
    "for classifier in classifiers:\n",
    "    performance_metrics[classifier.__class__.__name__] = train_and_evaluate(classifier, X_train_vec, y_train, X_test_vec, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier, metrics in performance_metrics.items():\n",
    "    print(f\"{classifier}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def evaluate_with_cross_val(classifier, X, y, n_splits=5):\n",
    "    scores = cross_val_score(classifier, X, y, cv=n_splits)\n",
    "    return np.mean(scores)\n",
    "\n",
    "X_vec = vectorizer.fit_transform(data['clean_text'])\n",
    "y = data['Category']\n",
    "\n",
    "for classifier in classifiers:\n",
    "    mean_score = evaluate_with_cross_val(classifier, X_vec, y)\n",
    "    print(f\"{classifier.__class__.__name__}: {mean_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_cm(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(cm_normalized, annot=True, cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "class_names = data['Category'].unique()\n",
    "\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train_vec, y_train)\n",
    "    y_pred = classifier.predict(X_test_vec)\n",
    "    print(f\"Confusion Matrix for {classifier.__class__.__name__}:\")\n",
    "    plot_cm(y_test, y_pred, class_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test_cosine_similarity_with_profiles function\n",
    "def test_cosine_similarity_with_profiles(text, vectorizer, category_profiles):\n",
    "    text_vec = vectorizer.transform([text])\n",
    "    max_similarity = -1\n",
    "    predicted_category = None\n",
    "\n",
    "    for category, profile_keywords in category_profiles.items():\n",
    "        profile_vec = vectorizer.transform([' '.join(profile_keywords)])\n",
    "        similarity = cosine_similarity(text_vec, profile_vec)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            predicted_category = category\n",
    "\n",
    "    return predicted_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_profiles = {\n",
    "    'Sports': ['football', 'soccer', 'basketball', 'tennis', 'cricket', 'golf', 'rugby', 'athletics', 'swimming', 'baseball', 'hockey', 'olympics'],\n",
    "    'Politics': ['election', 'government', 'policy', 'legislation', 'parliament', 'president', 'prime-minister', 'congress', 'senate', 'vote', 'party', 'diplomacy'],\n",
    "    'Business': ['finance', 'economy', 'stock', 'market', 'investment', 'trade', 'banking', 'corporation', 'revenue', 'profit', 'loss', 'growth', 'startup'],\n",
    "    'Entertainment': ['movie', 'music', 'television', 'celebrity', 'actor', 'actress', 'singer', 'festival', 'award', 'concert', 'theater', 'art', 'culture'],\n",
    "    'Tech': ['technology', 'software', 'hardware', 'internet', 'computer', 'smartphone', 'artificial-intelligence', 'robotics', 'data', 'security', 'innovation', 'research'],\n",
    "    'Sports-Politics': ['football', 'soccer', 'basketball', 'election', 'government', 'policy', 'legislation', 'vote', 'party'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance using the keyword profiles\n",
    "y_pred_profiles = [test_cosine_similarity_with_profiles(text, vectorizer, category_profiles) for text in X_test]\n",
    "\n",
    "# Calculate accuracy and display results\n",
    "accuracy_profiles = accuracy_score(y_test, y_pred_profiles)\n",
    "print(f\"Accuracy with keyword profiles: {accuracy_profiles:.4f}\")\n",
    "\n",
    "print(\"Classification Report with keyword profiles:\\n\", classification_report(y_test, y_pred_profiles))\n",
    "\n",
    "cm_profiles = confusion_matrix(y_test, y_pred_profiles)\n",
    "print(\"Confusion Matrix with keyword profiles:\\n\", cm_profiles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfdml_plugin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8eb26be180a0cf5b5323ae355caa2b1523e608ab9e6cb89b14d3325352809724"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
